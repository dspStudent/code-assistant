{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import huggingface_hub\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "import pypdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"D:\\D\\Code-asistent\\src\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code files reading testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "\n",
    "class GenericFileLoader:\n",
    "    \"\"\"A generic file loader that uses the `unstructured` library to parse files.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load(self) -> list[Document]:\n",
    "        \"\"\"Load and parse the file using the `unstructured` library.\"\"\"\n",
    "        try:\n",
    "            # Use `unstructured` to parse the file\n",
    "            elements = partition(filename=self.file_path)\n",
    "\n",
    "            # Combine the parsed elements into a single text\n",
    "            content = \"\\n\".join([str(el) for el in elements])\n",
    "\n",
    "            # Return the content as a Document\n",
    "            return [Document(page_content=content, metadata={\"source\": self.file_path})]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {self.file_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def lazy_load(self) -> list[Document]:\n",
    "        \"\"\"Lazily load and parse the file using the `unstructured` library.\"\"\"\n",
    "        try:\n",
    "            # Use `unstructured` to parse the file\n",
    "            elements = partition(filename=self.file_path)\n",
    "\n",
    "            # Yield each element as a separate Document\n",
    "            for el in elements:\n",
    "                yield Document(page_content=str(el), metadata={\"source\": self.file_path})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {self.file_path}: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading file D:\\D\\Code-asistent\\src\\b.sql: Partitioning is not supported for the FileType.UNK file type.\n",
      "Error loading file D:\\D\\Code-asistent\\src\\application.properties: Partitioning is not supported for the FileType.UNK file type.\n",
      "Error loading file D:\\D\\Code-asistent\\src\\config.ini: Partitioning is not supported for the FileType.UNK file type.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading file D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb: Partitioning is not supported for the FileType.UNK file type.\n",
      "Error loading file D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb: Partitioning is not supported for the FileType.UNK file type.\n",
      "Error loading file D:\\D\\Code-asistent\\src\\package.json: Not a valid ndjson\n",
      "Error loading file D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb: Partitioning is not supported for the FileType.UNK file type.\n",
      "Error loading file D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb: Partitioning is not supported for the FileType.UNK file type.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DirectoryLoader with the GenericFileLoader\n",
    "loader = DirectoryLoader(\n",
    "    path,\n",
    "    glob=\"**/*\",  # Load all files\n",
    "    use_multithreading=True,\n",
    "    loader_cls=GenericFileLoader,  # Use the generic loader\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\a.yml'}, page_content='hlo:'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\main.py'}, page_content='import panads as pd'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\Main.java'}, page_content='import java.util.*'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\Main.java'}, page_content='class Main(){'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\Main.java'}, page_content='}')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 126.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 113 documents\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\b.sql ---\n",
      "select...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\config.ini ---\n",
      "[Global]\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\a.yml ---\n",
      "hlo: null\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import configparser\n",
    "from typing import Iterator, List, Optional\n",
    "\n",
    "# Extended file type mapping\n",
    "FILE_TYPE_MAPPING = {\n",
    "    # Config files\n",
    "    '.yml': 'yaml',\n",
    "    '.yaml': 'yaml',\n",
    "    '.ini': 'ini',\n",
    "    '.cfg': 'ini',\n",
    "    '.properties': 'properties',\n",
    "    \n",
    "    # Code files\n",
    "    '.py': 'code',\n",
    "    '.java': 'code',\n",
    "    '.sql': 'code',\n",
    "    '.js': 'code',\n",
    "    '.json': 'code',\n",
    "    '.ipynb': 'notebook',\n",
    "    '.md': 'markdown',\n",
    "    '.txt': 'text',\n",
    "    \n",
    "    # Data files\n",
    "    '.csv': 'csv',\n",
    "    '.tsv': 'csv',\n",
    "}\n",
    "\n",
    "class CodebaseLoader(BaseLoader):\n",
    "    \"\"\"LangChain-compatible codebase loader with full error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.file_type = self._get_file_type()\n",
    "\n",
    "    def _get_file_type(self) -> str:\n",
    "        ext = Path(self.file_path).suffix.lower()\n",
    "        return FILE_TYPE_MAPPING.get(ext, 'unknown')\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"Lazy load documents from file path\"\"\"\n",
    "        try:\n",
    "            if not Path(self.file_path).exists():\n",
    "                yield self._create_document(\"File not found\")\n",
    "                return\n",
    "\n",
    "            yield from self._load_based_on_type()\n",
    "            \n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"Loading error: {str(e)}\")\n",
    "\n",
    "    def _load_based_on_type(self) -> Iterator[Document]:\n",
    "        \"\"\"Route loading based on file type\"\"\"\n",
    "        if self.file_type == 'yaml':\n",
    "            yield from self._load_yaml()\n",
    "        elif self.file_type == 'ini':\n",
    "            yield from self._load_ini()\n",
    "        elif self.file_type == 'properties':\n",
    "            yield from self._load_properties()\n",
    "        elif self.file_type == 'notebook':\n",
    "            yield from self._load_notebook()\n",
    "        elif self.file_type == 'code':\n",
    "            yield from self._load_code_file()\n",
    "        elif self.file_type == 'csv':\n",
    "            yield from self._load_csv()\n",
    "        elif self.file_type in ['text', 'markdown']:\n",
    "            yield from self._load_text_file()\n",
    "        else:\n",
    "            yield from self._load_unknown_file()\n",
    "\n",
    "    def _create_document(self, content: str, metadata: Optional[dict] = None) -> Document:\n",
    "        \"\"\"Create Document with standard metadata\"\"\"\n",
    "        base_metadata = {\n",
    "            \"source\": self.file_path,\n",
    "            \"file_type\": self.file_type,\n",
    "            \"file_size\": Path(self.file_path).stat().st_size,\n",
    "            \"file_extension\": Path(self.file_path).suffix.lower()\n",
    "        }\n",
    "        if metadata:\n",
    "            base_metadata.update(metadata)\n",
    "        return Document(page_content=content, metadata=base_metadata)\n",
    "\n",
    "    def _load_yaml(self) -> Iterator[Document]:\n",
    "        \"\"\"Load YAML configuration files\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                data = yaml.safe_load(f)\n",
    "            yield self._create_document(yaml.dump(data))\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"YAML Error: {str(e)}\")\n",
    "\n",
    "    def _load_ini(self) -> Iterator[Document]:\n",
    "        \"\"\"Load INI configuration files\"\"\"\n",
    "        try:\n",
    "            parser = configparser.ConfigParser()\n",
    "            parser.read(self.file_path, encoding='utf-8')\n",
    "            content = \"\\n\".join(\n",
    "                f\"[{section}]\\n{'\\n'.join(f'{k} = {v}' for k, v in parser.items(section))}\"\n",
    "                for section in parser.sections()\n",
    "            )\n",
    "            yield self._create_document(content)\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"INI Error: {str(e)}\")\n",
    "\n",
    "    def _load_properties(self) -> Iterator[Document]:\n",
    "        \"\"\"Load Java properties files\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            yield self._create_document(content)\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"Properties Error: {str(e)}\")\n",
    "\n",
    "    def _load_notebook(self) -> Iterator[Document]:\n",
    "        \"\"\"Load Jupyter notebooks\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "                notebook = json.load(f)\n",
    "\n",
    "            for idx, cell in enumerate(notebook.get('cells', [])):\n",
    "                content = ''.join(cell.get('source', []))\n",
    "                metadata = {\n",
    "                    \"cell_type\": cell.get('cell_type', 'unknown'),\n",
    "                    \"cell_index\": idx,\n",
    "                    \"execution_count\": cell.get('execution_count'),\n",
    "                }\n",
    "                yield self._create_document(content, metadata)\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"Notebook Error: {str(e)}\")\n",
    "\n",
    "    def _load_code_file(self) -> Iterator[Document]:\n",
    "        \"\"\"Load source code files\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "            metadata = {\n",
    "                \"line_count\": len(content.split('\\n')),\n",
    "                \"language\": self.file_type\n",
    "            }\n",
    "            yield self._create_document(content, metadata)\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"Code Load Error: {str(e)}\")\n",
    "\n",
    "    def _load_csv(self) -> Iterator[Document]:\n",
    "        \"\"\"Load CSV/TSV files\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path)\n",
    "            content = df.to_markdown()\n",
    "            metadata = {\n",
    "                \"columns\": list(df.columns),\n",
    "                \"row_count\": len(df)\n",
    "            }\n",
    "            yield self._create_document(content, metadata)\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"CSV Error: {str(e)}\")\n",
    "\n",
    "    def _load_text_file(self) -> Iterator[Document]:\n",
    "        \"\"\"Load generic text files\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "            yield self._create_document(content)\n",
    "        except Exception as e:\n",
    "            yield self._create_document(f\"Text Load Error: {str(e)}\")\n",
    "\n",
    "    def _load_unknown_file(self) -> Iterator[Document]:\n",
    "        \"\"\"Handle unknown file types\"\"\"\n",
    "        try:\n",
    "            with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read(1024)  # Read first 1KB to check if text\n",
    "            yield self._create_document(content[:1024])\n",
    "        except:\n",
    "            yield self._create_document(\"Binary file content not extracted\")\n",
    "\n",
    "def load_codebase(root_dir: str) -> List[Document]:\n",
    "    \"\"\"Load entire codebase with error resilience\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        root_dir,\n",
    "        glob=\"**/[!.]*\",  # Exclude hidden files\n",
    "        loader_cls=CodebaseLoader,\n",
    "        recursive=True,\n",
    "        show_progress=True,\n",
    "        use_multithreading=True,\n",
    "        silent_errors=True\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    documents = load_codebase(path)\n",
    "    print(f\"Successfully loaded {len(documents)} documents\")\n",
    "    for doc in documents[:3]:  # Show first 3 documents as example\n",
    "        print(f\"\\n--- Document from {doc.metadata['source']} ---\")\n",
    "        print(doc.page_content[:200] + \"...\")  # Show first 200 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\a.yml ---\n",
      "hlo: null\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\application.properties ---\n",
      "spring=0100...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\b.sql ---\n",
      "select...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\config.ini ---\n",
      "[Global]\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\main.py ---\n",
      "import panads as pd\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\Main.java ---\n",
      "import java.util.*\n",
      "\n",
      "class Main(){\n",
      "    \n",
      "}...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\package.json ---\n",
      "{\n",
      "    \"hi\":\"hlo\"\n",
      "}...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb ---\n",
      "import langchain\n",
      "import huggingface_hub\n",
      "import os\n",
      "from langchain_huggingface import HuggingFaceEndpoint\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "from...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb ---\n",
      "## loading files...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb ---\n",
      "## ingecting into vectordb...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_asist_ver1.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "import langchain\n",
      "import huggingface_hub\n",
      "import os\n",
      "from langchain_huggingface import HuggingFaceEndpoint\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "from...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "api_key=\"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\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "model1=\"qwen-2.5-32b\"\n",
      "model2=\"qwen-2.5-coder-32b\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "## testing LLM...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "llm=ChatGroq(\n",
      "    api_key=api_key,\n",
      "    model=model2,\n",
      "    temperature=0\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "op=llm.invoke(\"Hi i will send you the some part of the code you need to correct the syntax\"\n",
      "\"sytem.out.println('hi')\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "print(op.content)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "## testing embeding models...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
      "model = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "from typing import List\n",
      "from langchain.embeddings.base import Embeddings\n",
      "from transformers import AutoModel, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "class CodeBERTEmbeddings(Embeddings):\n",
      "    def __init__(self):\n",
      " ...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "codebert = CodeBERTEmbeddings()\n",
      "\n",
      "code_embeddings = codebert.embed_documents([])...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "from langchain_huggingface import HuggingFaceEmbeddings...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "embeddings=HuggingFaceEmbeddings(\n",
      "    model_name=\"jinaai/jina-embeddings-v2-base-code\"\n",
      "    )...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "from sentence_transformers import SentenceTransformer\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "model = SentenceTransformer('microsoft/codebert-base', use_auth_token=False)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "# Custom class that implements the Embeddings interface\n",
      "class CodeBertEmbeddings(Embeddings):\n",
      "    def __init__(self, model):\n",
      "        self.model = model\n",
      "\n",
      "    def embed_documents(self, code_chunks: list...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "print(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "## testing vectorDb...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "from qdrant_client import QdrantClient\n",
      "from langchain_qdrant import QdrantVectorStore\n",
      "from qdrant_client.http.models import VectorParams, Distance...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "qdrant_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.TBwdW04b7GD9gmouLOlPEyOZNtuJji-9cfmtRg4zAvo\"\n",
      "end_point=\"https://c510483d-1bc3-42c3-8645-3bf7698b142d.us-east4-0.gcp.cloud.qdrant.io...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "qdrant_client = QdrantClient(\n",
      "    url=end_point, \n",
      "    api_key=qdrant_key\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "# qdrant_client.create_collection(\n",
      "#     collection_name=\"test1\",\n",
      "#     vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
      "# )...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "# qdrant_client.delete_collection(collection_name=\"test1\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "vector_store = QdrantVectorStore(\n",
      "    client=qdrant_client,\n",
      "    collection_name=\"test1\",\n",
      "    embedding=code_bert_embeddings,\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "ch=[Document(metadata={'name': 'Devi Sri Ranga Prasad Gudimetla', 'index': 'Output: \"other\"'}, page_content='G.Devi Sri Ranga Prasad \\nLinkedin: g-devi-sri-ranga-prasad                                ...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "vector_store.add_documents(ch)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "vector_store.similarity_search(\"which school he studeid\", k=1)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_models_test.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "import langchain\n",
      "import huggingface_hub\n",
      "import os\n",
      "from langchain_huggingface import HuggingFaceEndpoint\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "from...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "path=r\"D:\\D\\Code-asistent\\src\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "## code files reading testing...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "from unstructured.partition.auto import partition\n",
      "from langchain_core.documents import Document\n",
      "from pathlib import Path\n",
      "\n",
      "class GenericFileLoader:\n",
      "    \"\"\"A generic file loader that uses the `unstructu...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "# Initialize the DirectoryLoader with the GenericFileLoader\n",
      "loader = DirectoryLoader(\n",
      "    path,\n",
      "    glob=\"**/*\",  # Load all files\n",
      "    use_multithreading=True,\n",
      "    loader_cls=GenericFileLoader,  # Use...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "documents...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "from langchain_community.document_loaders import DirectoryLoader\n",
      "from langchain_core.documents import Document\n",
      "from langchain_community.document_loaders.base import BaseLoader\n",
      "from pathlib import Path...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\code_reading_testing_ver1.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# imports...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "import langchain\n",
      "import huggingface_hub\n",
      "import os\n",
      "from langchain_huggingface import HuggingFaceEndpoint\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "from...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "a=1 \n",
      "n=[a]...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "n...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "del os.environ['GROQ_API_KEY']...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "key=os.environ.get(\"GROQ_API_KEY\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "key=\"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\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "os.environ[\"GROQ_API_KEY\"]=\"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\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "llm = ChatGroq(\n",
      "    api_key=key,\n",
      "    model=\"qwen-\"\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "## testing Groq...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "prompt=ChatPromptTemplate.from_template(\"how won first {devi}\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "chain=prompt | llm...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "print(chain.invoke(\"odi\").content)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "## loading pdf and per processing...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "path=r\"D:\\my rag\\src\\data\\DevCv31july2024M.pdf\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "loader=PyPDFLoader(path)\n",
      "docs=[]\n",
      "doc_load=loader.lazy_load()\n",
      "for doc in doc_load:\n",
      "    docs.append(doc)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "text_split=RecursiveCharacterTextSplitter(\n",
      "    chunk_size=300,\n",
      "    chunk_overlap=20,\n",
      "    length_function=len,\n",
      "    is_separator_regex=False,\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "text_split.split_text(\"hlo hi\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "text=text_split.create_documents([docs[0].page_content])...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "meta_data_prompt = \"\"\" You are a text categorization model. Your task is to analyze the provided text and classify it based on what it indicates about the personality of the person being described. Id...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "meta_data_prompt_temp=ChatPromptTemplate.from_template(meta_data_prompt)\n",
      "meta_data_chain=meta_data_prompt_temp |llm\n",
      "def get_meta_data(user_prompt):\n",
      "    return meta_data_chain.invoke({\"input_text\":user...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "get_meta_data(\"one day his frnds got in to a fight and he coudent abele to do anything\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "chuncks=[]\n",
      "for doc in docs:\n",
      "    for chunck in text_split.split_text(doc.page_content):\n",
      "        meta_data=get_meta_data(chunck)\n",
      "        chuncks.append(Document(\n",
      "            page_content=chunck,\n",
      "       ...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "chuncks...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# Embedings ...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "from langchain_huggingface import HuggingFaceEmbeddings...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "embeddings=HuggingFaceEmbeddings(\n",
      "    model_name=\"jinaai/jina-embeddings-v2-base-en\" \n",
      "    )...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "embeddings.embed_query(\"Hi my name is devi\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# Create a vector store with a sample text\n",
      "from langchain_core.vectorstores import InMemoryVectorStore\n",
      "\n",
      "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
      "\n",
      "vectorsto...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# vector DB...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "from qdrant_client import QdrantClient\n",
      "from langchain_qdrant import QdrantVectorStore\n",
      "from qdrant_client.http.models import VectorParams, Distance...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "del os.environ[\"QDRANT_MYRAG_API\"]...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "os.environ.get(\"QDRANT_MYRAG_API\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "qdrant_client = QdrantClient(\n",
      "    url=\"https://dd22169f-0c45-4259-8909-8dcf31aead3e.us-east4-0.gcp.cloud.qdrant.io:6333\", \n",
      "    api_key=\"SY1sJVZJ0ABQX0fh1W5JHDLkY1fSV4187D1-eaEqolASRORXw6O6nA\"\n",
      ")\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "qdrant_client.create_collection(\n",
      "    collection_name=\"my_rag_col\",\n",
      "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "qdrant_client.create_collection(\n",
      "    collection_name=\"my_rag_user_train\",\n",
      "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "print(qdrant_client.get_collections())...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "qdrant_client.delete_collection(collection_name=\"my_rag_col\")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store = QdrantVectorStore(\n",
      "    client=qdrant_client,\n",
      "    collection_name=\"my_rag_col\",\n",
      "    embedding=embeddings,\n",
      ")...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store_user_train = QdrantVectorStore(\n",
      "    client=qdrant_client,\n",
      "    collection_name=\"my_rag_user_train\",\n",
      "    embedding=embeddings,\n",
      ")\n",
      "...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store_user_train.add_documents(chuncks)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store.add_documents(chuncks)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "chuncks...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store.similarity_search(\"which school he studeid\", k=1)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store_user_train.similarity_search(\"which school he studeid\", k=1)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# Rag...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "user_prompt_template=\"\"\" Based on the following context, answer the user's question exactly. Do not provide any extra or irrelevant information and also correct the grammer when you give output also d...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "prompt=ChatPromptTemplate.from_template(user_prompt_template)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "rag_chain=prompt|llm...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "def user_prompt_fun(rag_chain, user_prompt, vector_store):\n",
      "    retrival_data=vector_store.similarity_search(user_prompt)\n",
      "    print(rag_chain.invoke({\"context\":retrival_data, \"user_prompt\":user_prompt}...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "input=\"who is devi's best frnd\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "user_prompt_fun(rag_chain=rag_chain, user_prompt=input, vector_store=vector_store)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# Fine tunning...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "print(os.environ.get(\"QDRANT_MYRAG_API\"))...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "def fine_tunning_rag(user_input, vector_store):\n",
      "    meta_data_from_user=get_meta_data(user_prompt=user_input)\n",
      "    this_chunks=[]\n",
      "    for this_chunk in text_split.split_text(user_input):\n",
      "        this_c...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "user_input=\"devi's 7 frnds from betck are sai kiran , gnana prakash, sai sudakar, ravi teja, vinay, varshit, and shekar but shekar is not with us and lost connection with our gang\"...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "fine_tunning_rag(user_input=user_input, vector_store=vector_store)...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "# testing vector db...\n",
      "\n",
      "--- Document from D:\\D\\Code-asistent\\src\\notebooks\\test_llm.ipynb ---\n",
      "vector_store.similarity_search(\"how many frinds devi has and tell abot them\", k=1)...\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:  # Show first 3 documents as example\n",
    "        print(f\"\\n--- Document from {doc.metadata['source']} ---\")\n",
    "        print(doc.page_content[:200] + \"...\")  # Show first 200 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\b.sql', 'file_type': 'code', 'file_size': 6, 'file_extension': '.sql', 'line_count': 1, 'language': 'code'}, page_content='select'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\a.yml', 'file_type': 'yaml', 'file_size': 4, 'file_extension': '.yml'}, page_content='hlo: null\\n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\application.properties', 'file_type': 'properties', 'file_size': 11, 'file_extension': '.properties'}, page_content='spring=0100'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\config.ini', 'file_type': 'ini', 'file_size': 8, 'file_extension': '.ini'}, page_content='[Global]\\n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\Main.java', 'file_type': 'code', 'file_size': 44, 'file_extension': '.java', 'line_count': 5, 'language': 'code'}, page_content='import java.util.*\\n\\nclass Main(){\\n    \\n}'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\package.json', 'file_type': 'code', 'file_size': 20, 'file_extension': '.json', 'line_count': 3, 'language': 'code'}, page_content='{\\n    \"hi\":\"hlo\"\\n}'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_asist_ver1.ipynb', 'file_type': 'notebook', 'file_size': 2569, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 0, 'execution_count': 2}, page_content='import langchain\\nimport huggingface_hub\\nimport os\\nfrom langchain_huggingface import HuggingFaceEndpoint\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain_groq import ChatGroq\\nimport pypdf\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain.schema.document import Document\\nfrom langchain_community.document_loaders import DirectoryLoader'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_asist_ver1.ipynb', 'file_type': 'notebook', 'file_size': 2569, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 1, 'execution_count': None}, page_content='## loading files'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_asist_ver1.ipynb', 'file_type': 'notebook', 'file_size': 2569, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 2, 'execution_count': None}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_asist_ver1.ipynb', 'file_type': 'notebook', 'file_size': 2569, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 3, 'execution_count': None}, page_content='## ingecting into vectordb'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_asist_ver1.ipynb', 'file_type': 'notebook', 'file_size': 2569, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 4, 'execution_count': None}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_asist_ver1.ipynb', 'file_type': 'notebook', 'file_size': 2569, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 5, 'execution_count': None}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 0, 'execution_count': 30}, page_content='import langchain\\nimport huggingface_hub\\nimport os\\nfrom langchain_huggingface import HuggingFaceEndpoint\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain_groq import ChatGroq\\nimport pypdf\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain.schema.document import Document\\nfrom langchain_community.document_loaders import DirectoryLoader\\nfrom transformers import RobertaTokenizer, RobertaModel'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 1, 'execution_count': 2}, page_content='api_key=\"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\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 2, 'execution_count': 24}, page_content='model1=\"qwen-2.5-32b\"\\nmodel2=\"qwen-2.5-coder-32b\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 3, 'execution_count': None}, page_content='## testing LLM'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 4, 'execution_count': 25}, page_content='llm=ChatGroq(\\n    api_key=api_key,\\n    model=model2,\\n    temperature=0\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 5, 'execution_count': 28}, page_content='op=llm.invoke(\"Hi i will send you the some part of the code you need to correct the syntax\"\\n\"sytem.out.println(\\'hi\\')\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 6, 'execution_count': 29}, page_content='print(op.content)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 7, 'execution_count': None}, page_content='## testing embeding models'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 8, 'execution_count': None}, page_content='tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\\nmodel = RobertaModel.from_pretrained(\"microsoft/graphcodebert-base\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 9, 'execution_count': 46}, page_content='from typing import List\\nfrom langchain.embeddings.base import Embeddings\\nfrom transformers import AutoModel, AutoTokenizer\\nimport torch\\n\\nclass CodeBERTEmbeddings(Embeddings):\\n    def __init__(self):\\n        self.model_name = \"microsoft/codebert-base\"\\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\\n        self.model = AutoModel.from_pretrained(self.model_name)\\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n        self.model.to(self.device)\\n\\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n        embeddings = []\\n        for text in texts:\\n            inputs = self.tokenizer(\\n                text, \\n                return_tensors=\"pt\", \\n                padding=True, \\n                truncation=True, \\n                max_length=512\\n            ).to(self.device)\\n            \\n            with torch.no_grad():\\n                outputs = self.model(**inputs)\\n            \\n            # Use mean pooling for sentence embedding\\n            token_embeddings = outputs.last_hidden_state\\n            sentence_embedding = token_embeddings.mean(dim=1).squeeze()\\n            embeddings.append(sentence_embedding.cpu().numpy().tolist())\\n        \\n        return embeddings\\n\\n    def embed_query(self, text: str) -> List[float]:\\n        return self.embed_documents([text])[0]'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 10, 'execution_count': 51}, page_content='os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 11, 'execution_count': None}, page_content='codebert = CodeBERTEmbeddings()\\n\\ncode_embeddings = codebert.embed_documents([])'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 12, 'execution_count': 32}, page_content='from langchain_huggingface import HuggingFaceEmbeddings'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 13, 'execution_count': None}, page_content='embeddings=HuggingFaceEmbeddings(\\n    model_name=\"jinaai/jina-embeddings-v2-base-code\"\\n    )'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 14, 'execution_count': 69}, page_content='from sentence_transformers import SentenceTransformer\\n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 15, 'execution_count': 71}, page_content=\"model = SentenceTransformer('microsoft/codebert-base', use_auth_token=False)\"),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 16, 'execution_count': 72}, page_content='# Custom class that implements the Embeddings interface\\nclass CodeBertEmbeddings(Embeddings):\\n    def __init__(self, model):\\n        self.model = model\\n\\n    def embed_documents(self, code_chunks: list):\\n        embeddings = self.model.encode(code_chunks)  # Shape (n, 768) where n is the number of code chunks\\n        return embeddings.tolist()  # Convert numpy array to list of lists\\n\\n    def embed_query(self, query: str):\\n        return self.model.encode([query])[0].tolist()  # Embed a single query string\\n\\n# Create an instance of the custom embeddings class\\ncode_bert_embeddings = CodeBertEmbeddings(model)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 17, 'execution_count': 49}, page_content='print(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 18, 'execution_count': None}, page_content='## testing vectorDb'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 19, 'execution_count': 54}, page_content='from qdrant_client import QdrantClient\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client.http.models import VectorParams, Distance'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 20, 'execution_count': 56}, page_content='qdrant_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.TBwdW04b7GD9gmouLOlPEyOZNtuJji-9cfmtRg4zAvo\"\\nend_point=\"https://c510483d-1bc3-42c3-8645-3bf7698b142d.us-east4-0.gcp.cloud.qdrant.io\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 21, 'execution_count': 57}, page_content='qdrant_client = QdrantClient(\\n    url=end_point, \\n    api_key=qdrant_key\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 22, 'execution_count': None}, page_content='# qdrant_client.create_collection(\\n#     collection_name=\"test1\",\\n#     vectors_config=VectorParams(size=768, distance=Distance.COSINE)\\n# )'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 23, 'execution_count': None}, page_content='# qdrant_client.delete_collection(collection_name=\"test1\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 24, 'execution_count': 73}, page_content='vector_store = QdrantVectorStore(\\n    client=qdrant_client,\\n    collection_name=\"test1\",\\n    embedding=code_bert_embeddings,\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 25, 'execution_count': 77}, page_content='ch=[Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'Output: \"other\"\\'}, page_content=\\'G.Devi Sri Ranga Prasad \\\\nLinkedin: g-devi-sri-ranga-prasad                                                                                        Email: devisrprasad948@gmail.com\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills\"\\'}, page_content=\\'GitHub:-  https://github.com/dspStudent                                                            Mobile:  +91-7032857480 \\\\nSKILLS SUMMARY \\\\n• Languages: C++, C, Core Java, Python, R , kotlin \\\\n• Technologies: HTML , CSS, Java Script, Java Spring Boot, Spring Security 6\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'Based on the provided text, the content category is \"skills\". This is because the text mainly discusses the tools, platforms, and programming languages that the person is familiar with, which are indicative of their skills.\\'}, page_content=\\'• Tools/Platforms: MySQL, MongoDB \\\\n• Data Analysis:          Excel, Tabuleo  \\\\nINTERNSHIP  \\\\n• R Programing language:                                                                                                                             June 2023\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"projects\"\\'}, page_content=\\'PROJECTS \\\\n• Anime List Backend Api:\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"other\"\\'}, page_content=\\'• Api Link :- https://naa-anime-list-backend-api.onrender.com/login                                         \\\\n• Users can keep track of the anime they have watched\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills\"\\'}, page_content=\\'• The security is robust because it contains OAuth 2.0 and JWT token verification.  And also The Animes Collection has 14k+ \\\\nrecords   \\\\n• Developed proficiently in Spring Boot Security 6, with APIs and MongoDB as well. \\\\n• Rock paper scissor game:\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills, experiences\"\\'}, page_content=\\'• AI-based project where the player competes against the computer.                                                             Jul 2023 \\\\n• Utilizes OpenCV and Media Pipe for hand gesture recognition.\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills, experiences\"\\'}, page_content=\\'• Developed proficiency in Python and learned to implement computer vision and machine learning techniques. \\\\n• Airbnb data analysis: \\\\n• Conducted a comprehensive analysis of Airbnb pricing patterns to understand market dynamics.      Jun 2023\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills, experiences\"\\'}, page_content=\\'• Developed a predictive model capable of estimating Airbnb rental prices.  \\\\n• Enhanced data analytics skills, particularly in R programming, and gained insights into the factors influencing \\\\naccommodation pricing.  \\\\nACHIEVEMENTS\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills\"\\'}, page_content=\\'ACHIEVEMENTS  \\\\n• Solved 370+ Questions in LeetCode , 100+ Streak in LeetCode and GFG, CodeCheff Max Rating 1503,  GFG Max Rating 1780, \\\\nLeetCode Max Rating 1489 \\\\nCERTIFICATES \\\\n• R programing || Board infinity Jul 2023 \\\\n \\\\n• Java ||Hacker Rank  Feb 2022 \\\\n \\\\n• Python || Coursera   Nov 2022\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"experiences\"\\'}, page_content=\\'• C, C++ || Coursera Dec 2021 \\\\nEDUCATION \\\\nLovely Professional  University                                                               Phagwara, IN\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"experiences\"\\'}, page_content=\\'B.Tech. in Computer Science 76%                                                                                                                             2021-2025\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"other\"\\'}, page_content=\\'Narayana Junior College                                                                                                                                Andhra Pradesh, IN\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"other\"\\'}, page_content=\\'Intermediate 96.5%                                                                                                                                                    2019-2021\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills\"\\'}, page_content=\\'Narayana High School                                                                                                                                      Andhra Pradesh, IN   \\\\n     10th class 95% \\\\n      \\\\n• Embarked on a steep learning curve to master R programming and data analysis techniques.\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills, experiences\"\\'}, page_content=\\'• Overcame initial challenges through dedicated study and hands -on practice with real datasets.  \\\\n• Transitioned from finding data analysis challenging to enjoying the intricacies of data interpretation and model creation.\\'),\\n Document(metadata={\\'name\\': \\'Devi Sri Ranga Prasad Gudimetla\\', \\'index\\': \\'\"skills, behaviors\"\\'}, page_content=\\'• Applied newly acquired R programming skills to contribute meaningfully to a project, demonstrating the ability to learn  \\\\n• and apply new technologies effectively.     \\\\n \\\\n2018-2019\\')]'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 26, 'execution_count': 78}, page_content='vector_store.add_documents(ch)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 27, 'execution_count': 79}, page_content='vector_store.similarity_search(\"which school he studeid\", k=1)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_models_test.ipynb', 'file_type': 'notebook', 'file_size': 17993, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 28, 'execution_count': None}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 0, 'execution_count': None}, page_content='# imports'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 1, 'execution_count': 1}, page_content='import langchain\\nimport huggingface_hub\\nimport os\\nfrom langchain_huggingface import HuggingFaceEndpoint\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain_groq import ChatGroq\\nimport pypdf\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain.schema.document import Document'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 2, 'execution_count': 3}, page_content='a=1 \\nn=[a]'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 3, 'execution_count': 4}, page_content='n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 4, 'execution_count': 2}, page_content=\"del os.environ['GROQ_API_KEY']\"),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 5, 'execution_count': None}, page_content='key=os.environ.get(\"GROQ_API_KEY\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 6, 'execution_count': 2}, page_content='key=\"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\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 7, 'execution_count': None}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 8, 'execution_count': None}, page_content='os.environ[\"GROQ_API_KEY\"]=\"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\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 9, 'execution_count': None}, page_content='llm = ChatGroq(\\n    api_key=key,\\n    model=\"qwen-\"\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 10, 'execution_count': None}, page_content='## testing Groq'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 11, 'execution_count': None}, page_content='prompt=ChatPromptTemplate.from_template(\"how won first {devi}\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 12, 'execution_count': None}, page_content='chain=prompt | llm'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 13, 'execution_count': None}, page_content='print(chain.invoke(\"odi\").content)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 14, 'execution_count': None}, page_content='## loading pdf and per processing'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 15, 'execution_count': None}, page_content='path=r\"D:\\\\my rag\\\\src\\\\data\\\\DevCv31july2024M.pdf\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 16, 'execution_count': None}, page_content='loader=PyPDFLoader(path)\\ndocs=[]\\ndoc_load=loader.lazy_load()\\nfor doc in doc_load:\\n    docs.append(doc)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 17, 'execution_count': None}, page_content='text_split=RecursiveCharacterTextSplitter(\\n    chunk_size=300,\\n    chunk_overlap=20,\\n    length_function=len,\\n    is_separator_regex=False,\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 18, 'execution_count': None}, page_content='text_split.split_text(\"hlo hi\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 19, 'execution_count': None}, page_content='text=text_split.create_documents([docs[0].page_content])'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 20, 'execution_count': None}, page_content='meta_data_prompt = \"\"\" You are a text categorization model. Your task is to analyze the provided text and classify it based on what it indicates about the personality of the person being described. Identify key personality traits, behaviors, interests, emotional tendencies, or if it is a story about the person. Remember for every prompt give only  Return the result as a single string indicating the content category. This are one of the possible content categories are: \"personality traits\", \"behaviors\", \"interests\", \"emotional tendencies\", \"skills\", \"experiences\", \"hobbies\", \"story\", or \"other\". Example: Input: \"He is a kind and generous person.\" Output: \"personality traits\" Input: \"She loves playing the piano and enjoys reading books.\" Output: \"interests\" Input: \"He tends to be very organized and punctual.\" Output: \"behaviors\" Input: \"She often feels anxious in large crowds.\" Output: \"emotional tendencies\" Input: \"He is excellent at programming in Python.\" Output: \"skills\" Input: \"She traveled across Europe last summer.\" Output: \"experiences\" Input: \"He enjoys hiking and photography.\" Output: \"hobbies\" Input: \"Once upon a time, he decided to travel the world and had many adventures.\" Output: \"story\" Now, analyze the following text: \"{input_text}\" Return only the content category as a single string. \"\"\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 21, 'execution_count': None}, page_content='meta_data_prompt_temp=ChatPromptTemplate.from_template(meta_data_prompt)\\nmeta_data_chain=meta_data_prompt_temp |llm\\ndef get_meta_data(user_prompt):\\n    return meta_data_chain.invoke({\"input_text\":user_prompt}).content\\n    '),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 22, 'execution_count': None}, page_content='get_meta_data(\"one day his frnds got in to a fight and he coudent abele to do anything\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 23, 'execution_count': None}, page_content='chuncks=[]\\nfor doc in docs:\\n    for chunck in text_split.split_text(doc.page_content):\\n        meta_data=get_meta_data(chunck)\\n        chuncks.append(Document(\\n            page_content=chunck,\\n            metadata={\\n                \"name\":\"Devi Sri Ranga Prasad Gudimetla\",\\n                \"index\":meta_data\\n            }\\n        ))\\n        print(meta_data)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 24, 'execution_count': None}, page_content='chuncks'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 25, 'execution_count': None}, page_content='# Embedings '),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 26, 'execution_count': 5}, page_content='from langchain_huggingface import HuggingFaceEmbeddings'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 27, 'execution_count': None}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 28, 'execution_count': 6}, page_content='embeddings=HuggingFaceEmbeddings(\\n    model_name=\"jinaai/jina-embeddings-v2-base-en\" \\n    )'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 29, 'execution_count': None}, page_content='embeddings.embed_query(\"Hi my name is devi\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 30, 'execution_count': None}, page_content='# Create a vector store with a sample text\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\ntext = \"LangChain is the framework for building context-aware reasoning applications\"\\n\\nvectorstore = InMemoryVectorStore.from_texts(\\n    [text],\\n    embedding=embeddings,\\n)\\n\\n# Use the vectorstore as a retriever\\nretriever = vectorstore.as_retriever()\\n\\n# Retrieve the most similar text\\nretrieved_documents = retriever.invoke(\"What is LangChain?\")\\n\\n# show the retrieved document\\'s content\\nretrieved_documents[0].page_content'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 31, 'execution_count': None}, page_content='# vector DB'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 32, 'execution_count': None}, page_content='from qdrant_client import QdrantClient\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client.http.models import VectorParams, Distance'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 33, 'execution_count': None}, page_content='del os.environ[\"QDRANT_MYRAG_API\"]'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 34, 'execution_count': None}, page_content='os.environ.get(\"QDRANT_MYRAG_API\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 35, 'execution_count': None}, page_content='qdrant_client = QdrantClient(\\n    url=\"https://dd22169f-0c45-4259-8909-8dcf31aead3e.us-east4-0.gcp.cloud.qdrant.io:6333\", \\n    api_key=\"SY1sJVZJ0ABQX0fh1W5JHDLkY1fSV4187D1-eaEqolASRORXw6O6nA\"\\n)\\n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 36, 'execution_count': None}, page_content='qdrant_client.create_collection(\\n    collection_name=\"my_rag_col\",\\n    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 37, 'execution_count': None}, page_content='qdrant_client.create_collection(\\n    collection_name=\"my_rag_user_train\",\\n    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 38, 'execution_count': None}, page_content='print(qdrant_client.get_collections())'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 39, 'execution_count': None}, page_content='qdrant_client.delete_collection(collection_name=\"my_rag_col\")'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 40, 'execution_count': None}, page_content='vector_store = QdrantVectorStore(\\n    client=qdrant_client,\\n    collection_name=\"my_rag_col\",\\n    embedding=embeddings,\\n)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 41, 'execution_count': None}, page_content='vector_store_user_train = QdrantVectorStore(\\n    client=qdrant_client,\\n    collection_name=\"my_rag_user_train\",\\n    embedding=embeddings,\\n)\\n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 42, 'execution_count': None}, page_content='vector_store_user_train.add_documents(chuncks)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 43, 'execution_count': None}, page_content='vector_store.add_documents(chuncks)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 44, 'execution_count': None}, page_content='chuncks'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 45, 'execution_count': None}, page_content='vector_store.similarity_search(\"which school he studeid\", k=1)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 46, 'execution_count': None}, page_content='vector_store_user_train.similarity_search(\"which school he studeid\", k=1)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 47, 'execution_count': None}, page_content='# Rag'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 48, 'execution_count': None}, page_content='user_prompt_template=\"\"\" Based on the following context, answer the user\\'s question exactly. Do not provide any extra or irrelevant information and also correct the grammer when you give output also don\\'t mention based on doucmunet. Context: {context} Question: {user_prompt} Answer: \"\"\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 49, 'execution_count': None}, page_content='prompt=ChatPromptTemplate.from_template(user_prompt_template)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 50, 'execution_count': None}, page_content='rag_chain=prompt|llm'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 51, 'execution_count': None}, page_content='def user_prompt_fun(rag_chain, user_prompt, vector_store):\\n    retrival_data=vector_store.similarity_search(user_prompt)\\n    print(rag_chain.invoke({\"context\":retrival_data, \"user_prompt\":user_prompt}).content)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 52, 'execution_count': None}, page_content='input=\"who is devi\\'s best frnd\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 53, 'execution_count': None}, page_content='user_prompt_fun(rag_chain=rag_chain, user_prompt=input, vector_store=vector_store)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 54, 'execution_count': None}, page_content='# Fine tunning'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 55, 'execution_count': None}, page_content='print(os.environ.get(\"QDRANT_MYRAG_API\"))'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 56, 'execution_count': None}, page_content='def fine_tunning_rag(user_input, vector_store):\\n    meta_data_from_user=get_meta_data(user_prompt=user_input)\\n    this_chunks=[]\\n    for this_chunk in text_split.split_text(user_input):\\n        this_chunks.append(\\n            Document(\\n                page_content=user_input,\\n                metadata={\\n                    \"name\":\"Devi Sri Ranga Prasad\",\\n                    \"index\":meta_data_from_user\\n                }\\n            )\\n        )\\n    vector_store.add_documents(this_chunks)\\n    print(\"fine tunning done\")\\n'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 57, 'execution_count': None}, page_content='user_input=\"devi\\'s 7 frnds from betck are sai kiran , gnana prakash, sai sudakar, ravi teja, vinay, varshit, and shekar but shekar is not with us and lost connection with our gang\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 58, 'execution_count': None}, page_content='fine_tunning_rag(user_input=user_input, vector_store=vector_store)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 59, 'execution_count': None}, page_content='# testing vector db'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\test_llm.ipynb', 'file_type': 'notebook', 'file_size': 72784, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 60, 'execution_count': None}, page_content='vector_store.similarity_search(\"how many frinds devi has and tell abot them\", k=1)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\main.py', 'file_type': 'code', 'file_size': 56, 'file_extension': '.py', 'line_count': 4, 'language': 'code'}, page_content='import panads as pd\\n\\nfor i in range(10):\\n    print(i)'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 0, 'execution_count': 2}, page_content='import langchain\\nimport huggingface_hub\\nimport os\\nfrom langchain_huggingface import HuggingFaceEndpoint\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain.chains import LLMChain\\nfrom langchain_groq import ChatGroq\\nimport pypdf\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain.schema.document import Document\\nfrom langchain_community.document_loaders import DirectoryLoader'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 1, 'execution_count': 8}, page_content='path=r\"D:\\\\D\\\\Code-asistent\\\\src\"'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'markdown', 'cell_index': 2, 'execution_count': None}, page_content='## code files reading testing'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 3, 'execution_count': 13}, page_content='from unstructured.partition.auto import partition\\nfrom langchain_core.documents import Document\\nfrom pathlib import Path\\n\\nclass GenericFileLoader:\\n    \"\"\"A generic file loader that uses the `unstructured` library to parse files.\"\"\"\\n\\n    def __init__(self, file_path: str):\\n        self.file_path = file_path\\n\\n    def load(self) -> list[Document]:\\n        \"\"\"Load and parse the file using the `unstructured` library.\"\"\"\\n        try:\\n            # Use `unstructured` to parse the file\\n            elements = partition(filename=self.file_path)\\n\\n            # Combine the parsed elements into a single text\\n            content = \"\\\\n\".join([str(el) for el in elements])\\n\\n            # Return the content as a Document\\n            return [Document(page_content=content, metadata={\"source\": self.file_path})]\\n        except Exception as e:\\n            print(f\"Error loading file {self.file_path}: {e}\")\\n            return []\\n\\n    def lazy_load(self) -> list[Document]:\\n        \"\"\"Lazily load and parse the file using the `unstructured` library.\"\"\"\\n        try:\\n            # Use `unstructured` to parse the file\\n            elements = partition(filename=self.file_path)\\n\\n            # Yield each element as a separate Document\\n            for el in elements:\\n                yield Document(page_content=str(el), metadata={\"source\": self.file_path})\\n        except Exception as e:\\n            print(f\"Error loading file {self.file_path}: {e}\")\\n            return []'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 4, 'execution_count': 30}, page_content='# Initialize the DirectoryLoader with the GenericFileLoader\\nloader = DirectoryLoader(\\n    path,\\n    glob=\"**/*\",  # Load all files\\n    use_multithreading=True,\\n    loader_cls=GenericFileLoader,  # Use the generic loader\\n)\\n\\n# Load the documents\\ndocuments = loader.load()'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 5, 'execution_count': 29}, page_content='documents'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 6, 'execution_count': 36}, page_content='from langchain_community.document_loaders import DirectoryLoader\\nfrom langchain_core.documents import Document\\nfrom langchain_community.document_loaders.base import BaseLoader\\nfrom pathlib import Path\\nimport json\\nimport pandas as pd\\nimport yaml\\nimport configparser\\nfrom typing import Iterator, List, Optional\\n\\n# Extended file type mapping\\nFILE_TYPE_MAPPING = {\\n    # Config files\\n    \\'.yml\\': \\'yaml\\',\\n    \\'.yaml\\': \\'yaml\\',\\n    \\'.ini\\': \\'ini\\',\\n    \\'.cfg\\': \\'ini\\',\\n    \\'.properties\\': \\'properties\\',\\n    \\n    # Code files\\n    \\'.py\\': \\'code\\',\\n    \\'.java\\': \\'code\\',\\n    \\'.sql\\': \\'code\\',\\n    \\'.js\\': \\'code\\',\\n    \\'.json\\': \\'code\\',\\n    \\'.ipynb\\': \\'notebook\\',\\n    \\'.md\\': \\'markdown\\',\\n    \\'.txt\\': \\'text\\',\\n    \\n    # Data files\\n    \\'.csv\\': \\'csv\\',\\n    \\'.tsv\\': \\'csv\\',\\n}\\n\\nclass CodebaseLoader(BaseLoader):\\n    \"\"\"LangChain-compatible codebase loader with full error handling\"\"\"\\n    \\n    def __init__(self, file_path: str):\\n        self.file_path = file_path\\n        self.file_type = self._get_file_type()\\n\\n    def _get_file_type(self) -> str:\\n        ext = Path(self.file_path).suffix.lower()\\n        return FILE_TYPE_MAPPING.get(ext, \\'unknown\\')\\n\\n    def lazy_load(self) -> Iterator[Document]:\\n        \"\"\"Lazy load documents from file path\"\"\"\\n        try:\\n            if not Path(self.file_path).exists():\\n                yield self._create_document(\"File not found\")\\n                return\\n\\n            yield from self._load_based_on_type()\\n            \\n        except Exception as e:\\n            yield self._create_document(f\"Loading error: {str(e)}\")\\n\\n    def _load_based_on_type(self) -> Iterator[Document]:\\n        \"\"\"Route loading based on file type\"\"\"\\n        if self.file_type == \\'yaml\\':\\n            yield from self._load_yaml()\\n        elif self.file_type == \\'ini\\':\\n            yield from self._load_ini()\\n        elif self.file_type == \\'properties\\':\\n            yield from self._load_properties()\\n        elif self.file_type == \\'notebook\\':\\n            yield from self._load_notebook()\\n        elif self.file_type == \\'code\\':\\n            yield from self._load_code_file()\\n        elif self.file_type == \\'csv\\':\\n            yield from self._load_csv()\\n        elif self.file_type in [\\'text\\', \\'markdown\\']:\\n            yield from self._load_text_file()\\n        else:\\n            yield from self._load_unknown_file()\\n\\n    def _create_document(self, content: str, metadata: Optional[dict] = None) -> Document:\\n        \"\"\"Create Document with standard metadata\"\"\"\\n        base_metadata = {\\n            \"source\": self.file_path,\\n            \"file_type\": self.file_type,\\n            \"file_size\": Path(self.file_path).stat().st_size,\\n            \"file_extension\": Path(self.file_path).suffix.lower()\\n        }\\n        if metadata:\\n            base_metadata.update(metadata)\\n        return Document(page_content=content, metadata=base_metadata)\\n\\n    def _load_yaml(self) -> Iterator[Document]:\\n        \"\"\"Load YAML configuration files\"\"\"\\n        try:\\n            with open(self.file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n                data = yaml.safe_load(f)\\n            yield self._create_document(yaml.dump(data))\\n        except Exception as e:\\n            yield self._create_document(f\"YAML Error: {str(e)}\")\\n\\n    def _load_ini(self) -> Iterator[Document]:\\n        \"\"\"Load INI configuration files\"\"\"\\n        try:\\n            parser = configparser.ConfigParser()\\n            parser.read(self.file_path, encoding=\\'utf-8\\')\\n            content = \"\\\\n\".join(\\n                f\"[{section}]\\\\n{\\'\\\\n\\'.join(f\\'{k} = {v}\\' for k, v in parser.items(section))}\"\\n                for section in parser.sections()\\n            )\\n            yield self._create_document(content)\\n        except Exception as e:\\n            yield self._create_document(f\"INI Error: {str(e)}\")\\n\\n    def _load_properties(self) -> Iterator[Document]:\\n        \"\"\"Load Java properties files\"\"\"\\n        try:\\n            with open(self.file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n                content = f.read()\\n            yield self._create_document(content)\\n        except Exception as e:\\n            yield self._create_document(f\"Properties Error: {str(e)}\")\\n\\n    def _load_notebook(self) -> Iterator[Document]:\\n        \"\"\"Load Jupyter notebooks\"\"\"\\n        try:\\n            with open(self.file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n                notebook = json.load(f)\\n\\n            for idx, cell in enumerate(notebook.get(\\'cells\\', [])):\\n                content = \\'\\'.join(cell.get(\\'source\\', []))\\n                metadata = {\\n                    \"cell_type\": cell.get(\\'cell_type\\', \\'unknown\\'),\\n                    \"cell_index\": idx,\\n                    \"execution_count\": cell.get(\\'execution_count\\'),\\n                }\\n                yield self._create_document(content, metadata)\\n        except Exception as e:\\n            yield self._create_document(f\"Notebook Error: {str(e)}\")\\n\\n    def _load_code_file(self) -> Iterator[Document]:\\n        \"\"\"Load source code files\"\"\"\\n        try:\\n            with open(self.file_path, \\'r\\', encoding=\\'utf-8\\', errors=\\'replace\\') as f:\\n                content = f.read()\\n            metadata = {\\n                \"line_count\": len(content.split(\\'\\\\n\\')),\\n                \"language\": self.file_type\\n            }\\n            yield self._create_document(content, metadata)\\n        except Exception as e:\\n            yield self._create_document(f\"Code Load Error: {str(e)}\")\\n\\n    def _load_csv(self) -> Iterator[Document]:\\n        \"\"\"Load CSV/TSV files\"\"\"\\n        try:\\n            df = pd.read_csv(self.file_path)\\n            content = df.to_markdown()\\n            metadata = {\\n                \"columns\": list(df.columns),\\n                \"row_count\": len(df)\\n            }\\n            yield self._create_document(content, metadata)\\n        except Exception as e:\\n            yield self._create_document(f\"CSV Error: {str(e)}\")\\n\\n    def _load_text_file(self) -> Iterator[Document]:\\n        \"\"\"Load generic text files\"\"\"\\n        try:\\n            with open(self.file_path, \\'r\\', encoding=\\'utf-8\\', errors=\\'replace\\') as f:\\n                content = f.read()\\n            yield self._create_document(content)\\n        except Exception as e:\\n            yield self._create_document(f\"Text Load Error: {str(e)}\")\\n\\n    def _load_unknown_file(self) -> Iterator[Document]:\\n        \"\"\"Handle unknown file types\"\"\"\\n        try:\\n            with open(self.file_path, \\'r\\', encoding=\\'utf-8\\', errors=\\'replace\\') as f:\\n                content = f.read(1024)  # Read first 1KB to check if text\\n            yield self._create_document(content[:1024])\\n        except:\\n            yield self._create_document(\"Binary file content not extracted\")\\n\\ndef load_codebase(root_dir: str) -> List[Document]:\\n    \"\"\"Load entire codebase with error resilience\"\"\"\\n    loader = DirectoryLoader(\\n        root_dir,\\n        glob=\"**/[!.]*\",  # Exclude hidden files\\n        loader_cls=CodebaseLoader,\\n        recursive=True,\\n        show_progress=True,\\n        use_multithreading=True,\\n        silent_errors=True\\n    )\\n    return loader.load()\\n\\nif __name__ == \"__main__\":\\n    # Example usage\\n    documents = load_codebase(path)\\n    print(f\"Successfully loaded {len(documents)} documents\")\\n    for doc in documents[:3]:  # Show first 3 documents as example\\n        print(f\"\\\\n--- Document from {doc.metadata[\\'source\\']} ---\")\\n        print(doc.page_content[:200] + \"...\")  # Show first 200 chars'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 7, 'execution_count': 37}, page_content='for doc in documents:  # Show first 3 documents as example\\n        print(f\"\\\\n--- Document from {doc.metadata[\\'source\\']} ---\")\\n        print(doc.page_content[:200] + \"...\")  # Show first 200 chars'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 8, 'execution_count': 38}, page_content='documents'),\n",
       " Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\notebooks\\\\code_reading_testing_ver1.ipynb', 'file_type': 'notebook', 'file_size': 99541, 'file_extension': '.ipynb', 'cell_type': 'code', 'cell_index': 9, 'execution_count': None}, page_content='')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='import panads as pd\n",
      "\n",
      "for i in range(10):\n",
      "    print(i)' metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\main.py', 'file_type': 'code', 'file_size': 56, 'file_extension': '.py', 'line_count': 4, 'language': 'code'}\n"
     ]
    }
   ],
   "source": [
    "print( Document(metadata={'source': 'D:\\\\D\\\\Code-asistent\\\\src\\\\main.py', 'file_type': 'code', 'file_size': 56, 'file_extension': '.py', 'line_count': 4, 'language': 'code'}, page_content='import panads as pd\\n\\nfor i in range(10):\\n    print(i)'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector db ingectiomn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [3:13:42<00:00, 1056.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client.http.models import VectorParams, Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.TBwdW04b7GD9gmouLOlPEyOZNtuJji-9cfmtRg4zAvo\"\n",
    "end_point=\"https://c510483d-1bc3-42c3-8645-3bf7698b142d.us-east4-0.gcp.cloud.qdrant.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\n",
    "    url=end_point, \n",
    "    api_key=qdrant_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.create_collection(\n",
    "    collection_name=\"test1\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D\\Code-asistent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\D\\Code-asistent\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:195: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  warnings.warn(\n",
      "No sentence-transformers model found with name microsoft/codebert-base. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('microsoft/codebert-base', use_auth_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# Custom class that implements the Embeddings interface\n",
    "class CodeBertEmbeddings(Embeddings):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, code_chunks: list):\n",
    "        embeddings = self.model.encode(code_chunks)  # Shape (n, 768) where n is the number of code chunks\n",
    "        return embeddings.tolist()  # Convert numpy array to list of lists\n",
    "\n",
    "    def embed_query(self, query: str):\n",
    "        return self.model.encode([query])[0].tolist()  # Embed a single query string\n",
    "\n",
    "# Create an instance of the custom embeddings class\n",
    "code_bert_embeddings = CodeBertEmbeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"test1\",\n",
    "    embedding=code_bert_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dafe232afb4f4db081109631fbfb0bba',\n",
       " 'b2e1b46f575f494389c1a20c4342d25c',\n",
       " '1048ea532581444596ad0c995b78fb62',\n",
       " '9b0d7e067a604ade9ecd847ba27cc7ea',\n",
       " 'd1a69fc5cb1c42398b2eae37b818a509',\n",
       " '407eaa87b7bb48ea924900722e948ecd',\n",
       " '483cf23902464bf3baa673c597ffae9e',\n",
       " '5320b8e9a89a47a0bdac83d180933093',\n",
       " '917d39d3bdbb4c678ac98fbd8fca05c5',\n",
       " '56e259884230491cb9fe6a67e76fd54f',\n",
       " '0d169d4135cd4ad58d1eefa1884ab3f6',\n",
       " '3df39c487d354479a0bf1d6cd08a1494',\n",
       " '5d31251c16a94725985b04c172d0d0d7',\n",
       " '4ba3b411f4c24e30818ebad0a961eacb',\n",
       " 'bec9c412ae174e1bab87e636d1e1acd3',\n",
       " 'efa82fb323fa4d67b9dbaf223dbd5c6d',\n",
       " '570decff90fe4205a2b7e8179ac26f2d',\n",
       " '7e05ba58b55e4b42aa5d6642eacffee0',\n",
       " '6a3b1ad98d08440697cc34a1f3a6d8c2',\n",
       " '5eb1efdcf50e4fcd9291806727b801ba',\n",
       " 'a5747cff9e1245fa876398f0fb4a8125',\n",
       " '801cae6a355f4a9a8d924afa4a5261fd',\n",
       " 'ce8136b359314e78913bd6c27ad2b2cc',\n",
       " '03986118b37b446096956b8d4a885e56',\n",
       " 'f795fbecca9a4ceaaee1366609eac5ea',\n",
       " 'ec851b3a5f254b57b39c608b03e467a6',\n",
       " 'fa8def2ab437471c8753985ed809b9f0',\n",
       " '267755c0b8c74404a1e9ee948ef7df8e',\n",
       " '4d8032caad31429c997a0a447ee04a5a',\n",
       " '164418e292b74da1a6e853c8de8d4b40',\n",
       " '8f45ae19fb48494c9590730a905b0062',\n",
       " '525c4a88113a4d80b5789775dc2b5d40',\n",
       " '14ed4ac2e5d442f8b48609b505f15554',\n",
       " 'b4bca6428c5a43149ab021322ef6cb22',\n",
       " 'a7bb8784a61e4f81bc241bb5d0ed5873',\n",
       " 'c0c05e97b8c947d1977459646443fc07',\n",
       " 'cd3329f8e566490491c08a8a099f06ab',\n",
       " '61c1f057222949d39ba676fe8e704c81',\n",
       " '39e965b86edf493bb69bbb6f68f76c43',\n",
       " 'e193a3d1cafa483086065798a1d70128',\n",
       " 'ca437b08abc34a5bbf8e524c08701b41',\n",
       " 'f519cd4a89d8485f946db35ea48ba9b7',\n",
       " '21f61464606b4b78bcabd4967b31f004',\n",
       " 'c87076e0fa1044b0b684774f135310d9',\n",
       " '72adb192b1f5445c8f8b8edf0c9f95ae',\n",
       " 'c94e5a06b9d94cd3af73d0c07bd319f1',\n",
       " 'd12c450a0c0d4b4798a224a96a9cb201',\n",
       " 'e5eae0256476444cbc4af6f0e3affdd7',\n",
       " '198879bfa5f84cceaaa3f50be959f9c6',\n",
       " 'cd94d4a4e14b48bf8240ec57f8ff4db9',\n",
       " '4db1c8a1df814e1682c62d1a447adaa9',\n",
       " '1aea68df8cf242eea56643941f69f4bd',\n",
       " '7a9aa1a71e3c44588cd387404d8c84b9',\n",
       " '88c7fcbcf59542f1a2452a2ffa94121b',\n",
       " '0079a5d9d89f4b958ceeac889a25de4c',\n",
       " 'c83c06041442461499b14f1e449f7e9b',\n",
       " '3bc1d4e7633b483a9ae199aae547bc1d',\n",
       " '518f4664cac3499285c199c5e406c9b3',\n",
       " '44a6ccce711740e1bf70d0f298d3e555',\n",
       " '481f45e2b3734c3bb8cc6d80c5068eea',\n",
       " '8bc5f7b293c74f4ba46c623d60890a15',\n",
       " '92f3b7478dc748538ce5d143ed6b1f18',\n",
       " 'e40cb4dd7f72455d8c11311b69e903fa',\n",
       " 'c49cddb5c92f48dd83bd9d71b2ff6d87',\n",
       " 'ad33e10303dd47d6ba9aee98fb50c611',\n",
       " 'a03956da540b4eb3857844a03e1f6a46',\n",
       " '4987b0ec3e404806abb5201c88f73a0e',\n",
       " 'd348454772f94bbf923e930facce73db',\n",
       " '2a5acd587e4f4c5bbdab25057ead375f',\n",
       " '72f4213abd314a989bdf9a5bc5981d8f',\n",
       " '16e581edf6f14f22a9e9fde03d00c469',\n",
       " '2c1b9704950e49c1905295c1f79cf93a',\n",
       " '9c28b579bea5484bb1368d82b7f94514',\n",
       " 'f0feb2d5f2b04ef68f673ad4c033be7d',\n",
       " '74d8598505f141cc939236152c05513b',\n",
       " '6400c5b826ee4e319d82bf3cead12d67',\n",
       " 'ee970bafbe764374bc0bec7dbfc34853',\n",
       " '59afcfa31c4f4f159156560728b7ba17',\n",
       " 'a618005523f5416da001dc488bc439a6',\n",
       " '01cc4ea9a3c746a385c70daa4d69a5d6',\n",
       " '48803e7de58a4f4b9fb4d4dbbf471685',\n",
       " '197f799374f64a73b263409b0c48f3fa',\n",
       " 'db9a22b471ba49b8ada5958bddf1b7a6',\n",
       " '86ddcb7edd4741659859a4c1afb4dc41',\n",
       " '7fdcb33ed5684cd992344412dcf203fd',\n",
       " 'ab9978d3011749b0979902b70e4e3f42',\n",
       " '651d0c855acc48e083f85f74205363d0',\n",
       " 'f59f19d255c54bc9989938d7d4f510e1',\n",
       " '368ed0e419cf4920bfea18a4b98a7f5e',\n",
       " '0314fb586ba649e7befd8cc3c12beb87',\n",
       " '096c828e43f841a3865794038132b969',\n",
       " '6e05651180344e29bea45908f17516e5',\n",
       " '8e63cf81db684837b9da79304da56a77',\n",
       " '6ea1f0cb1f97432c9378985d200beca6',\n",
       " '054f4a1f5c204d9594f0b9b3dd397851',\n",
       " '73807dfec7e14f0cb703afd00d4797fe',\n",
       " '0b1774e1e975427eb6300616cb21c0dc',\n",
       " '96f912069d8446e3a4ad7d10eaa6939d',\n",
       " '809e1d6b54ce4ec0b96fc63aaf2cb857',\n",
       " '6f79be4d38ba441cafea0dddf148ea50',\n",
       " '1ad6701608854793b04c09eb618df05b',\n",
       " '1340535b7f474c9bbcda330b1010e252',\n",
       " 'a1ef5767486e4d53a3743cf0f986e819',\n",
       " 'e4a7aa8062fe44659fa626b3da550f2b',\n",
       " '76727496b41f4f51a9d455d72d467963',\n",
       " 'f47ca3d1168c4246a1e55bf29208d5a5',\n",
       " '93e4255c04d74f3b821e754fc8c9bf91',\n",
       " 'e86894a5f3aa4607872758982473ffda',\n",
       " '85a0ae6696ce497884783222fcfe9c79',\n",
       " 'd9f0c143685b4e3eb953a355a95518e6',\n",
       " 'b5f16cb08a704984af64f063e8f1f212',\n",
       " '67192c33485e4c58a68d90762115d61a',\n",
       " 'b29ca90e0c314691a3299f1cb4c58dd6']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VectorStore.search() missing 1 required positional argument: 'search_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMain.java\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: VectorStore.search() missing 1 required positional argument: 'search_type'"
     ]
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
